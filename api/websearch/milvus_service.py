import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
from typing import List, Optional
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
from pymilvus import (
    connections, FieldSchema, CollectionSchema,
    DataType, Collection, utility
)
from configs.settings import *
from src.models.reranker_model import *


class MilvusService:
    def __init__(
            self,
            collection_name: str = "test",
            dim: int = 1024,
            host: str = "localhost",
            port: str = "19530",
            overwrite: bool = False,
            openai_base_url: str = MODEL_API_BASE,
            openai_api_key: str = MODEL_API_KEY,
            embedding_model: str = EMBEDDING_MODEL,
            reranker_key: str = "siliconflow/bge-reranker-v2-m3",
            reranker_local_path: str = MODEL_RERANKER_PATH,
            reranker_model: str = 'BAAI/bge-reranker-v2-m3',
    ):
        """
        åˆå§‹åŒ– Milvus å‘é‡å­˜å‚¨
        å‚æ•°:
            collection_name: é›†åˆåç§°
            dim: å‘é‡ç»´åº¦
            host: Milvus æœåŠ¡å™¨åœ°å€
            port: Milvus ç«¯å£
            overwrite: æ˜¯å¦è¦†ç›–ç°æœ‰é›†åˆ
            openai_api_key: OpenAI API å¯†é’¥
            embedding_model: OpenAI åµŒå…¥æ¨¡å‹åç§°
        """
        # è¿æ¥ Milvus
        connections.connect(host=host, port=port)

        self.collection_name = collection_name
        self.dim = dim

        self.embedder = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_base=openai_base_url,
            openai_api_key=openai_api_key,
            chunk_size=32
        )
        self.reranker = RerankerWrapper(
            reranker_key=reranker_key,
            model_name=reranker_model,
            local_path=reranker_local_path,
            device="cpu"
        )
        # å®šä¹‰é›†åˆç»“æ„
        self.fields = [
            FieldSchema(name="pk", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),
            FieldSchema(name="metadata", dtype=DataType.JSON),
            FieldSchema(name="text_length", dtype=DataType.INT64)
        ]

        self.schema = CollectionSchema(
            fields=self.fields,
            description="LangChain Documents with OpenAI Embeddings",
            enable_dynamic_field=True
        )

        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        if utility.has_collection(collection_name):
            if overwrite:
                utility.drop_collection(collection_name)
                print(f"å·²è¦†ç›–ç°æœ‰é›†åˆ: {collection_name}")
                self.collection = self._create_collection()
            else:
                self.collection = Collection(collection_name)
                print(f"å·²åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
        else:
            self.collection = self._create_collection()

        # åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not self.collection.has_index():
            self._create_index()

    def _create_collection(self) -> Collection:
        """åˆ›å»ºæ–°é›†åˆ"""
        print(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}")
        return Collection(
            name=self.collection_name,
            schema=self.schema,
            consistency_level="Strong"
        )

    def _create_index(self, index_params: Optional[dict] = None):
        """åˆ›å»ºå‘é‡ç´¢å¼• (ä¼˜åŒ–ç”¨äº OpenAI åµŒå…¥)"""
        default_index = {
            "index_type": "IVF_FLAT",
            "metric_type": "COSINE",  # OpenAI æ¨èä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            "params": {"nlist": 16}  # è¾ƒå¤§å€¼æé«˜æœç´¢ç²¾åº¦
        }
        params = index_params or default_index

        print(f"åˆ›å»ºç´¢å¼•: {params}")
        self.collection.create_index(
            field_name="embedding",
            index_params=params
        )
        self.collection.load()

    def insert_documents(self, documents: List[Document], batch_size: int = 32):
        """
        æ’å…¥ LangChain æ–‡æ¡£åˆ—è¡¨ (è‡ªåŠ¨ç”Ÿæˆ OpenAI åµŒå…¥)
        å‚æ•°:
            documents: LangChain Document å¯¹è±¡åˆ—è¡¨
            batch_size: åˆ†æ‰¹å¤„ç†å¤§å° (é¿å…APIé™æµ)
        """
        total_docs = len(documents)
        print(f"å¼€å§‹æ’å…¥ {total_docs} ä¸ªæ–‡æ¡£ (åˆ†æ‰¹å¤§å°: {batch_size})")

        for i in range(0, total_docs, batch_size):
            batch_docs = documents[i:i + batch_size]
            # ç”ŸæˆåµŒå…¥
            texts = [doc.page_content for doc in batch_docs]
            embeddings = self.embedder.embed_documents(texts)
            text_length = [len(doc.page_content) for doc in batch_docs]
            # å‡†å¤‡æ•°æ®
            metadatas = [doc.metadata for doc in batch_docs]

            # æ’å…¥æ‰¹æ¬¡æ•°æ®
            entities = [
                texts,
                embeddings,
                metadatas,
                text_length
            ]

            try:
                self.collection.insert(entities)
                print(f"å·²æ’å…¥ {min(i + batch_size, total_docs)}/{total_docs} æ–‡æ¡£")
            except Exception as e:
                print(f"æ’å…¥å¤±è´¥ (æ–‡æ¡£ {i}-{i + batch_size}): {str(e)}")
                raise

        self.collection.flush()
        print(f"æ–‡æ¡£æ’å…¥å®Œæˆ! æ€»è®¡: {self.collection.num_entities} ä¸ªæ–‡æ¡£")

    def similarity_search(
            self,
            query: str,
            k: int = 3,
            rerank: bool = True,
            **search_kwargs
    ) -> List[Document]:
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedder.embed_query(query)

        # æ„å»ºé•¿åº¦è¿‡æ»¤è¡¨è¾¾å¼
        length_filter = "text_length > 50"

        # æœç´¢å‚æ•°
        search_params = {
            "metric_type": "COSINE",
            "params": {"nprobe": 8, "ef": 64},
            **search_kwargs
        }

        # æ‰§è¡Œæœç´¢
        search_result = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=k * 5 if rerank else k,  # æ‰©å¤§æ£€ç´¢é‡ç¡®ä¿ç»“æœå……è¶³
            expr=length_filter,  # å…³é”®ä¿®æ”¹ï¼šæ•°æ®åº“å±‚è¿‡æ»¤
            output_fields=["text", "metadata"],
            consistency_level="Strong"
        )

        candidates = []
        for hits in search_result:
            for hit in hits:
                text = getattr(hit.entity, "text", "")
                metadata = getattr(hit.entity, "metadata", {})

                doc = Document(
                    page_content=text,
                    metadata={
                        **metadata,
                        "distance": hit.distance,
                        "raw_score": 1 - hit.distance  # å­˜å‚¨åŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°
                    }
                )
                candidates.append(doc)

        # å¦‚æœæ²¡æœ‰å¯ç”¨rerankï¼Œç›´æ¥è¿”å›å‰kä¸ªç»“æœ
        if not rerank or len(candidates) <= k:
            return candidates[:k]
        # æ‰§è¡Œrerank
        return self.rerank_documents(query, candidates, k)

    def rerank_documents(self, query: str, candidates: List[Document], k: int) -> List[Document]:
        print("ğŸ¯ æ­£åœ¨è°ƒç”¨ reranker å¯¹å€™é€‰æ–‡æ¡£é‡æ–°æ’åº...")
        docs = [doc.page_content for doc in candidates]
        scores = self.reranker.run(query, docs, normalize=True)

        for doc, score in zip(candidates, scores):
            doc.metadata["rerank_score"] = float(score)

        candidates.sort(key=lambda x: x.metadata["rerank_score"], reverse=True)
        return candidates[:k]

    def hybrid_search(
            self,
            query: str,
            k: int = 5,
            filter_expr: Optional[str] = None,
            keyword_weight: float = 0.3,
            vector_weight: float = 0.7
    ) -> List[Document]:
        """
        æ··åˆæœç´¢ (ç»“åˆå…³é”®è¯å’Œå‘é‡ç›¸ä¼¼åº¦)
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            filter_expr: å…ƒæ•°æ®è¿‡æ»¤è¡¨è¾¾å¼
            keyword_weight: å…³é”®è¯åˆ†æ•°æƒé‡
            vector_weight: å‘é‡åˆ†æ•°æƒé‡
        """
        # å‘é‡æœç´¢
        vector_results = self.similarity_search(query, k * 2)

        # å…³é”®è¯æœç´¢ (ç®€å•å®ç°)
        query_lower = query.lower()

        def keyword_score(text):
            words = set(query_lower.split())
            text_words = set(text.lower().split())
            return len(words & text_words) / len(words) if words else 0

        # åˆå¹¶ç»“æœ
        scored_docs = []
        for doc in vector_results:
            vector_score = 1 - doc.metadata.get("distance", 0)  # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦
            text_score = keyword_score(doc.page_content)
            combined_score = (vector_score * vector_weight) + (text_score * keyword_weight)
            scored_docs.append((combined_score, doc))

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        scored_docs.sort(reverse=True, key=lambda x: x[0])
        return [doc for _, doc in scored_docs[:k]]

    def clear_collection(self):
        """æ¸…ç©ºé›†åˆæ•°æ®"""
        utility.drop_collection(self.collection_name)
        self.collection = self._create_collection()
        self._create_index()
        print("é›†åˆå·²æ¸…ç©º")

    def close(self):
        """å…³é—­è¿æ¥"""
        connections.disconnect(alias="one")
        print("Milvus è¿æ¥å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–
    vector_store = MilvusService(
        collection_name="test",
        overwrite=True,
        # embedding_model="bge-m3-pro",
        # reranker_key="siliconflow/bge-reranker-v2-m3",  # æˆ– "local/bge-reranker-v2-m3"
        # reranker_model="BAAI/bge-reranker-v2-m3"  # æˆ–æœ¬åœ°è·¯å¾„
    )

    # ç¤ºä¾‹æ–‡æ¡£
    docs = [
        Document(
            page_content='#\n\n  * ç”µè§†åŠ¨ç”»ç³»åˆ—\n  * ç”µå½±\n  * å•†å“\n  * æ¸¸æˆ\n  * é›†æ¢å¼å¡ç‰Œæ¸¸æˆ\n  * å®å¯æ¢¦å›¾é‰´\n\nClose\n\nå®å¯æ¢¦å›¾é‰´\n\n0024 é˜¿æŸæ€ª\n\n0025\n\nçš®å¡ä¸˜\n\nçš®å¡ä¸˜ 0025\n\nå±æ€§\n\nç”µ\n\nå¼±ç‚¹\n\nåœ°é¢\n\nèº«é«˜ 0.4 m\n\nåˆ†ç±» é¼ å®å¯æ¢¦\n\nä½“é‡ 6.0 kg\n\næ€§åˆ« /\n\nç‰¹æ€§ é™ç”µ\n\nç‰¹æ€§ é™ç”µ èº«ä¸Šå¸¦æœ‰é™ç”µï¼Œæœ‰æ—¶ä¼šè®©æ¥è§¦åˆ°çš„å¯¹æ‰‹éº»ç—¹ã€‚\n\nå›¾é‰´ç‰ˆæœ¬\n\nä¸¤é¢Šä¸Šæœ‰å‚¨å­˜ç”µåŠ›çš„å›Šè¢‹ã€‚ä¸€æ—¦ç”Ÿæ°”å°±ä¼šæŠŠå‚¨å­˜çš„ç”µåŠ›ä¸€å£æ°”é‡Šæ”¾å‡ºæ¥ã€‚\n\næ®è¯´å½“å¥½å‡ åªèšåœ¨ä¸€èµ·æ—¶ï¼Œé‚£é‡Œå°±ä¼šå‡èšå¼ºçƒˆçš„ç”µåŠ›ï¼Œè¿˜å¯èƒ½ä¼šè½ä¸‹é—ªç”µã€‚\n\nåŒé¢Šæœ‰å›Šï¼Œç”¨ä»¥ç§¯è“„ç”µåŠ›ã€‚æ –æ¯åœ¨æ£®æ—ï¼Œæ€§æƒ…èªæ…§ï¼Œä¼šä»¥ç”µå‡»çƒ§ç¼åšç¡¬çš„æ ‘æœé£Ÿç”¨ã€‚\n\nèƒ½åŠ›\n\nHP\n\næ”»å‡»\n\né˜²å¾¡\n\nç‰¹æ”»\n\nç‰¹é˜²\n\né€Ÿåº¦\n\næ ·å­\n\n0025 çš®å¡ä¸˜\n\nç”µ\n\n0025 çš®å¡ä¸˜ è¶…æå·¨åŒ–\n\nç”µ\n\nè¿›åŒ–\n\n0172\n\nçš®ä¸˜\n\nç”µ\n\n0025\n\nçš®å¡ä¸˜\n\nç”µ\n\n0026\n\né›·ä¸˜\n\nç”µ\n\n0026\n\né›·ä¸˜\n\né˜¿ç½—æ‹‰çš„æ ·å­\n\nç”µ\n\nè¶…èƒ½åŠ›\n\n  * \n\nè¿”å›PokÃ©dex',
            metadata={'uuid': 'e8926baa84557853ea8df3288161e77c',
                      'title': 'çš®å¡ä¸˜| å®å¯æ¢¦å›¾é‰´| The official PokÃ©mon Website in China',
                      'snippet': 'çš®å¡ä¸˜. çš®å¡ä¸˜ 0025. å±æ€§. ç”µ. å¼±ç‚¹. åœ°é¢. èº«é«˜ 0.4 m. åˆ†ç±» é¼ å®å¯æ¢¦. ä½“é‡ 6.0 kg. æ€§åˆ« /. ç‰¹æ€§ ... 0025 çš®å¡ä¸˜ è¶…æå·¨åŒ–. ç”µ. è¿›åŒ–. 0172. çš®ä¸˜. ç”µ Â· 0025. çš®å¡ä¸˜.',
                      'link': 'https://www.pokemon.cn/play/pokedex/0025', 'score': 0.04201680672268904}
        ),
        Document(
            page_content='#\n\n  * é›»è¦–å‹•ç•«ç³»åˆ—\n  * é›»å½±\n  * å•†å“\n  * æ‡‰ç”¨ç¨‹å¼\n  * éŠæˆ²\n  * æ´»å‹•\n  * å¡ç‰ŒéŠæˆ²\n  * å¯¶å¯å¤¢åœ–é‘‘\n\nClose\n\nå¯¶å¯å¤¢åœ–é‘‘\n\n0024 é˜¿æŸæ€ª\n\n0025\n\nçš®å¡ä¸˜\n\nçš®å¡ä¸˜ 0025\n\nå±¬æ€§\n\né›»\n\nå¼±é»\n\nåœ°é¢\n\nèº«é«˜ 0.4 m\n\nåˆ†é¡ é¼ å¯¶å¯å¤¢\n\né«”é‡ 6.0 kg\n\næ€§åˆ¥ /\n\nç‰¹æ€§ éœé›»\n\nç‰¹æ€§ éœé›» èº«ä¸Šå¸¶æœ‰éœé›»ï¼Œæœ‰æ™‚æœƒä»¤æ¥è§¸åˆ°çš„å°æ‰‹éº»ç—ºã€‚\n\nåœ–é‘‘ç‰ˆæœ¬\n\né›™é °ä¸Šæœ‰å„²å­˜é›»åŠ›çš„å›Šè¢‹ã€‚ä¸€æ—¦ç”Ÿæ°£å°±æœƒæŠŠå„²å­˜çš„é›»åŠ›ä¸€å£æ°£é‡‹æ”¾å‡ºä¾†ã€‚\n\næ“šèªªåŒä¸€è™•æœ‰å¥½å¹¾éš»çš„æ™‚å€™ï¼Œé‚£è£¡å°±æœƒå‡é›†èµ·å¼·çƒˆçš„é›»åŠ›ï¼Œé‚„å¯èƒ½é€ æˆé–ƒé›»è½æ–¼è©²è™•ã€‚\n\né›™é °æœ‰å›Šï¼Œç”¨ä»¥ç©è“„é›»åŠ›ã€‚æ£²æ¯åœ¨æ£®æ—ï¼Œæ€§æƒ…è°æ…§ï¼Œæœƒä»¥é›»æ“Šç‡’ç¼å …ç¡¬çš„æ¨¹æœé£Ÿç”¨ã€‚\n\nèƒ½åŠ›\n\nHP\n\næ”»æ“Š\n\né˜²ç¦¦\n\nç‰¹æ”»\n\nç‰¹é˜²\n\né€Ÿåº¦\n\næ¨£å­\n\n0025 çš®å¡ä¸˜\n\né›»\n\n0025 çš®å¡ä¸˜ è¶…æ¥µå·¨åŒ–\n\né›»\n\né€²åŒ–\n\n0172\n\nçš®ä¸˜\n\né›»\n\n0025\n\nçš®å¡ä¸˜\n\né›»\n\n0026\n\né›·ä¸˜\n\né›»\n\n0026\n\né›·ä¸˜\n\né˜¿ç¾…æ‹‰çš„æ¨£å­\n\né›»\n\nè¶…èƒ½åŠ›\n\n  *   * \n\nè¿”å›PokÃ©dex',
            metadata={'uuid': '27998f19ce9eeee669d077a9546a9c78',
                      'title': 'çš®å¡ä¸˜| å¯¶å¯å¤¢åœ–é‘‘| The official PokÃ©mon Website in Taiwan',
                      'snippet': 'çš®å¡ä¸˜. çš®å¡ä¸˜ 0025. å±¬æ€§. é›». å¼±é». åœ°é¢. èº«é«˜ 0.4 m. åˆ†é¡ é¼ å¯¶å¯å¤¢. é«”é‡ 6.0 kg. æ€§åˆ¥ /. ç‰¹æ€§ ... 0025 çš®å¡ä¸˜ è¶…æ¥µå·¨åŒ–. é›». é€²åŒ–. 0172. çš®ä¸˜. é›» Â· 0025. çš®å¡ä¸˜.',
                      'link': 'https://tw.portal-pokemon.com/play/pokedex/0025', 'score': 0.03361344537815125}
        ),
        Document(
            page_content="çš®å¡ä¸˜çš„è¿›åŒ–æ˜¯é›·ä¸˜ï¼Œå®ƒä»¬éƒ½æ˜¯ç”µå±æ€§å®å¯æ¢¦ã€‚é›·ä¸˜æ¯”çš®å¡ä¸˜ä½“å‹æ›´å¤§ï¼Œæ”»å‡»åŠ›ä¹Ÿæ›´å¼ºã€‚",
            metadata={"uuid": "test-001", "title": "çš®å¡ä¸˜è¿›åŒ–ä»‹ç»"}
        ),
        Document(
            page_content="çš®ä¸˜æ˜¯çš®å¡ä¸˜çš„é€€åŒ–å½¢æ€ï¼Œå½“ä¸è®­ç»ƒå¸ˆå»ºç«‹è¶³å¤Ÿçš„äº²å¯†å…³ç³»åï¼Œçš®ä¸˜ä¼šè¿›åŒ–æˆçš®å¡ä¸˜ã€‚",
            metadata={"uuid": "test-002", "title": "çš®ä¸˜ä¸çš®å¡ä¸˜"}
        ),
        Document(
            page_content="å“ˆåˆ©æ³¢ç‰¹æ˜¯ä¸€éƒ¨éå¸¸è‘—åçš„é­”æ³•å°è¯´ï¼Œä¸å®å¯æ¢¦æ— å…³ã€‚",
            metadata={"uuid": "test-003", "title": "å“ˆåˆ©æ³¢ç‰¹"}
        ),
        Document(
            page_content="å¤©æ°”æ™´æœ—ï¼Œé€‚åˆéƒŠæ¸¸æˆ–è€…åœ¨å…¬å›­æ•£æ­¥ã€‚",
            metadata={"uuid": "test-004", "title": "å¤©æ°”ä¸å‡ºè¡Œ"}
        ),
        Document(
            page_content="é›·ä¸˜è¿˜æœ‰é˜¿ç½—æ‹‰å½¢æ€ï¼Œæ˜¯çš®å¡ä¸˜çš„ç‰¹æ®Šè¿›åŒ–ä¹‹ä¸€ã€‚",
            metadata={"uuid": "test-005", "title": "é›·ä¸˜é˜¿ç½—æ‹‰å½¢æ€"}
        ),
    ]

    # æ’å…¥æ–‡æ¡£ (è‡ªåŠ¨ç”ŸæˆåµŒå…¥)
    vector_store.insert_documents(docs)

    # ç›¸ä¼¼æ€§æœç´¢
    query = "çš®å¡ä¸˜çš„è¿›åŒ–æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"\nQuery: {query}")
    results = vector_store.similarity_search(query, k=1)

    for i, doc in enumerate(results):
        print(f"\nResult {i + 1}:")
        print(f"Content: {doc.page_content}")
        print(f"Metadata: {doc.metadata}")
        print(f"Similarity: {1 - doc.metadata.get('distance', 0):.3f}")

    # æ··åˆæœç´¢ç¤ºä¾‹
    # print("\nHybrid Search Results:")
    # hybrid_results = vector_store.hybrid_search(query, k=2)
    # for doc in hybrid_results:
    #     print(f"- {doc.page_content[:60]}...")

    # æ¸…ç†
    vector_store.close()
